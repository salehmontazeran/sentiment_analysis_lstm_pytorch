{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c61b007d-b6f2-4e44-ad9d-b17a2eb422fe",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49a7e39c-95dc-4811-b325-36243a4e6694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import copy\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataloader import default_collate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adecc40-4a13-42b3-b2df-4475909a8532",
   "metadata": {},
   "source": [
    "## Some useful constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20d92b82-4267-4cd7-8a5f-67d7e0e0faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_SIZE = 300\n",
    "PAD_TOKEN = '<PAD>'\n",
    "UNK_TOKEN = '<UNK>'\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "TEST_EVAL_BATCH_SIZE = 32\n",
    "# You can define your own constant in here\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2697fea-9c12-4ab7-bb4f-68881db3b471",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160b7647-70ce-4c3f-a1e9-9311eb68c637",
   "metadata": {},
   "source": [
    "This data is based on\n",
    "<a href=\"http://www.cs.cornell.edu/people/pabo/movie-review-data/\">this link</a>\n",
    "and contains movie reviews sentiment-analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7131e553-3b17-4cef-b940-7b79f0b45a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train lenght is: 8000\n",
      "eval lenght is: 2000\n",
      "test lenght is: 662\n"
     ]
    }
   ],
   "source": [
    "with open('./dataset.json') as f:\n",
    "    all_dataset = json.load(f)\n",
    "    \n",
    "for section in all_dataset.keys():\n",
    "    l = len(all_dataset[section])\n",
    "    print(f\"{section} lenght is: {l}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226ae1ee-8c58-4701-99ef-344a0a4c6d97",
   "metadata": {},
   "source": [
    "## Download and extract the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de951110-ed5c-4983-84f9-7986a864b870",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d20001-b797-4fbd-ae78-b0e53c5456f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./glove.6B.zip\n",
      "  inflating: ./glove/glove.6B.50d.txt  \n",
      "  inflating: ./glove/glove.6B.100d.txt  \n",
      "  inflating: ./glove/glove.6B.200d.txt  \n",
      "  inflating: ./glove/glove.6B.300d.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip ./glove.6B.zip -d \"./glove/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695db153-b2e6-4650-a0f1-f46e62995061",
   "metadata": {},
   "source": [
    "## Create embedding matrix and useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d14e34c-032d-4ba0-b55c-e3abb84194a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "emb_list = []\n",
    "with open(f'./glove/glove.6B.{EMB_SIZE}d.txt','r') as f:\n",
    "    for line in f.read().strip().split('\\n'):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        emb = values[1:]\n",
    "        word_list.append(word)\n",
    "        emb_list.append(emb)\n",
    "        \n",
    "emb_matrix = np.array(emb_list, 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acf153cb-e1f6-4e51-8e4d-c3380f145a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialize <UNK> token as an average of all embedings\n",
    "unk_emb = np.mean(emb_matrix, axis=0, keepdims=True)\n",
    "word_list.append(UNK_TOKEN)\n",
    "emb_matrix = np.vstack((emb_matrix, unk_emb))\n",
    "\n",
    "# We initialize <PAD> token as zeroes\n",
    "pad_emb = np.zeros((1, EMB_SIZE))\n",
    "word_list.append(PAD_TOKEN)\n",
    "emb_matrix = np.vstack((emb_matrix, pad_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28be1a5a-1f6a-4185-9479-ff687013baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_map = {word: id for (id, word) in enumerate(word_list)}\n",
    "\n",
    "def word_to_ids(word: str) -> list:\n",
    "    word = word.strip()\n",
    "    if word == \"\":\n",
    "        return []\n",
    "    if word in reverse_map:\n",
    "        return [reverse_map[word]]\n",
    "    elif word[-3:] in [\"n't\", \"'re\"]:\n",
    "        return word_to_ids(word[:-3]) + word_to_ids(word[-3:])\n",
    "    elif word[-2:] in [\"'s\", \"'d\", \"'m\"]:\n",
    "        return word_to_ids(word[:-2]) + word_to_ids(word[-2:])\n",
    "    else:\n",
    "        word = word.replace(\"'\", \"\")\n",
    "        if word in reverse_map:\n",
    "            return [reverse_map[word]]\n",
    "    return [reverse_map[UNK_TOKEN]]\n",
    "    \n",
    "def id_to_word(id: int) -> str:\n",
    "    return word_list[id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c3f21c-b625-4461-b307-9dcfb14bd163",
   "metadata": {},
   "source": [
    "## Tokenizer and sentence useful tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27aeef5d-8ef8-4bc9-8514-9447dab88e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(sentence: str) -> list:\n",
    "    sentence = sentence.strip()\n",
    "    return re.split(\"[ -]+\", sentence)\n",
    "\n",
    "def sentence_to_ids(sentence: str) -> list:\n",
    "    return sum(map(word_to_ids, tokenizer(sentence)), [])\n",
    "\n",
    "def ids_to_sentence(ids: list) -> list:\n",
    "    return ' '.join(map(id_to_word, ids))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e368e5a",
   "metadata": {},
   "source": [
    "## Build embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b1ac873-86c1-4276-83e5-287dd0676fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_len = len(word_list)\n",
    "weights_matrix = np.zeros((matrix_len, EMB_SIZE))\n",
    "words_found = 0\n",
    "\n",
    "for i, word in enumerate(word_list):\n",
    "    try: \n",
    "        weights_matrix[i] = emb_matrix[word_to_ids(word)]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(EMB_SIZE, ))\n",
    "\n",
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.size()\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        print(\"NOT TRAIN EMBED\")\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77307025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        x, trg = batch\n",
    "        optimizer.zero_grad()\n",
    "        x = x.type(torch.LongTensor).to(device)\n",
    "        output = model(x)\n",
    "\n",
    "        loss = criterion(output, trg.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            x, trg = batch\n",
    "            x = x.type(torch.LongTensor).to(device)\n",
    "            output = model(x)\n",
    "\n",
    "            loss = criterion(output, trg.to(device))\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def get_all_targets_and_predicted(model, iterator):\n",
    "    all_trg = []\n",
    "    all_prd = []\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            x, trg = batch\n",
    "            x = x.type(torch.LongTensor).to(device)\n",
    "            output = model(x)\n",
    "            \n",
    "            prd = output.argmax(1).tolist()\n",
    "            \n",
    "            all_trg += trg\n",
    "            all_prd += prd\n",
    "    return all_trg, all_prd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd1acd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_size, num_layers, output_size, train_embedding=False):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, not train_embedding)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).requires_grad_().to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).requires_grad_().to(device)\n",
    "        x = self.embedding(x)\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        out = out[:, -1,:]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebf67777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(review, seq_length):\n",
    "    review_len = len(review)\n",
    "    \n",
    "    if review_len <= seq_length:\n",
    "        zeroes = list(np.zeros(seq_length-review_len))\n",
    "        new = zeroes+list(review)\n",
    "    elif review_len > seq_length:\n",
    "        new = review[0:seq_length]    \n",
    "    return np.array(new, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72da1839",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Method2Dataset(Dataset):\n",
    "    def __init__(self, datadict):\n",
    "        self.data = [(pad_features(sentence_to_ids(sentence), 50), semantic) for sentence, semantic in datadict]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "train_dataset = Method2Dataset(all_dataset['train'])\n",
    "eval_dataset = Method2Dataset(all_dataset['eval'])\n",
    "test_dataset = Method2Dataset(all_dataset['test'])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=TEST_EVAL_BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=TEST_EVAL_BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6060997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "hidden_size = 100\n",
    "num_layers = 2\n",
    "output_size = 2\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "def train_and_report(weights_matrix, train_embed):\n",
    "    weights_matrix = torch.tensor(weights_matrix).clone()\n",
    "    lstm = LSTM(weights_matrix, hidden_size, num_layers, output_size, train_embed).float().to(device)\n",
    "    optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate, weight_decay=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_model_wight = None\n",
    "    min_loss = 1e8\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(lstm, train_dataloader, optimizer, criterion)\n",
    "        valid_loss = evaluate(lstm, eval_dataloader, criterion)\n",
    "\n",
    "        if min_loss > valid_loss:\n",
    "            min_loss = valid_loss\n",
    "            best_model_wight = copy.deepcopy(lstm.state_dict())\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02}')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f}')\n",
    "    \n",
    "    # Load best model\n",
    "    lstm.load_state_dict(best_model_wight)\n",
    "    \n",
    "    print('__________________TRAIN DATASET__________________')\n",
    "    trg, prd = get_all_targets_and_predicted(lstm, train_dataloader)\n",
    "    print(classification_report(trg, prd))\n",
    "    print('__________________EVAL DATASET__________________')\n",
    "    trg, prd = get_all_targets_and_predicted(lstm, eval_dataloader)\n",
    "    print(classification_report(trg, prd))\n",
    "    print('__________________TEST DATASET__________________')\n",
    "    trg, prd = get_all_targets_and_predicted(lstm, test_dataloader)\n",
    "    print(classification_report(trg, prd))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fd2ea52",
   "metadata": {},
   "source": [
    "## Freeze embeding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a69e1e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT TRAIN EMBED\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.582\n",
      "\t Val. Loss: 0.528\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.495\n",
      "\t Val. Loss: 0.498\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.471\n",
      "\t Val. Loss: 0.483\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.457\n",
      "\t Val. Loss: 0.501\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.431\n",
      "\t Val. Loss: 0.479\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.420\n",
      "\t Val. Loss: 0.458\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.400\n",
      "\t Val. Loss: 0.466\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.386\n",
      "\t Val. Loss: 0.469\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.365\n",
      "\t Val. Loss: 0.459\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.344\n",
      "\t Val. Loss: 0.493\n",
      "__________________TRAIN DATASET__________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.83      4000\n",
      "           1       0.84      0.81      0.83      4000\n",
      "\n",
      "    accuracy                           0.83      8000\n",
      "   macro avg       0.83      0.83      0.83      8000\n",
      "weighted avg       0.83      0.83      0.83      8000\n",
      "\n",
      "__________________EVAL DATASET__________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.80      0.78       993\n",
      "           1       0.79      0.75      0.77       991\n",
      "\n",
      "    accuracy                           0.78      1984\n",
      "   macro avg       0.78      0.78      0.78      1984\n",
      "weighted avg       0.78      0.78      0.78      1984\n",
      "\n",
      "__________________TEST DATASET__________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.80      0.78       320\n",
      "           1       0.79      0.76      0.77       320\n",
      "\n",
      "    accuracy                           0.78       640\n",
      "   macro avg       0.78      0.78      0.78       640\n",
      "weighted avg       0.78      0.78      0.78       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_report(weights_matrix, False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f54665ae",
   "metadata": {},
   "source": [
    "## Train Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e60454bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.572\n",
      "\t Val. Loss: 0.520\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.471\n",
      "\t Val. Loss: 0.479\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.409\n",
      "\t Val. Loss: 0.493\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.349\n",
      "\t Val. Loss: 0.499\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.281\n",
      "\t Val. Loss: 0.535\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.221\n",
      "\t Val. Loss: 0.598\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.190\n",
      "\t Val. Loss: 0.689\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.147\n",
      "\t Val. Loss: 0.640\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.136\n",
      "\t Val. Loss: 0.848\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.099\n",
      "\t Val. Loss: 0.754\n",
      "__________________TRAIN DATASET__________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.91      0.88      4000\n",
      "           1       0.90      0.84      0.87      4000\n",
      "\n",
      "    accuracy                           0.87      8000\n",
      "   macro avg       0.88      0.87      0.87      8000\n",
      "weighted avg       0.88      0.87      0.87      8000\n",
      "\n",
      "__________________EVAL DATASET__________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.83      0.79       990\n",
      "           1       0.81      0.73      0.77       994\n",
      "\n",
      "    accuracy                           0.78      1984\n",
      "   macro avg       0.78      0.78      0.78      1984\n",
      "weighted avg       0.78      0.78      0.78      1984\n",
      "\n",
      "__________________TEST DATASET__________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.80      0.77       319\n",
      "           1       0.78      0.71      0.74       321\n",
      "\n",
      "    accuracy                           0.75       640\n",
      "   macro avg       0.76      0.75      0.75       640\n",
      "weighted avg       0.76      0.75      0.75       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_and_report(weights_matrix, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "799ebefd249ff07842d0405c69fc025c2003392c4552a67b7d45f14b19200a81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
